{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pasos generales que seguiremos:\n",
    "    - Preparación de datos: Cargar y preprocesar las imágenes.\n",
    "    - Construcción del modelo: Crear una red neuronal convolucional (CNN) o usar un modelo preentrenado (transfer learning).\n",
    "    - Entrenamiento: Entrenar el modelo con los datos.\n",
    "    - Guardar el modelo: Exportar el modelo entrenado para su uso futuro.\n",
    "    - Evaluación: Probar el modelo con datos nuevos.\n",
    "\n",
    "2. Herramientas que usaremos:\n",
    "    - TensorFlow/Keras: Para construir y entrenar el modelo.\n",
    "    - OpenCV/PIL: Para el procesamiento de imágenes.\n",
    "    - NumPy: Para manipulación de datos.\n",
    "    - Matplotlib/Seaborn: Para visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias necesarias:\n",
    "\n",
    "- pip install pandas\n",
    "- pip install scikit-learn\n",
    "- pip install imblearn\n",
    "- pip install seaborn\n",
    "- pip install torch\n",
    "- pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Lista de encabezados\n",
    "1. Piezas del Vehículo:\n",
    "- 1:\"Antiniebla delantero derecho\", 2:\"Antiniebla delantero izquierdo\", 3:\"Asiento\", 4:\"Brazo del techo\", 5:\"Brazo transversal\", 6:\"Capó\", 7:\"Cerradura capo\", 8:\"Cerradura Maletero\", 9:\"Cerradura puerta\", 10:\"Espejo lateral derecho\", 11:\"Espejo lateral izquierdo\", 12:\"Espejo retrovisor\", 13:\"Faros derecho\", 14:\"Faros izquierdo\", 15:\"Guardabarros delantero derecho\", 16:\"Guardabarros delantero izquierdo\", \n",
    "17:\"Guardabarros trasero derecho\", 18:\"Guardabarros trasero izquierdo\", 19:\"Limpiaparabrisas\", 20:\"Luz indicadora delantera derecha\", 21:\"Luz indicadora delantera izquierda\", 22:\"Luz indicadora trasera derecha\", 23:\"Luz indicadora trasera izquierda\", 24:\"Luz trasera derecho\", \n",
    "25:\"Luz trasera izquierdo\", 26:\"Maletero\", 27:\"Manija derecha\", 28:\"Manija izquierda\", 29:\"Marco de la ventana\", 30:\"Marco de las puertas\", 31:\"Matrícula\", 32:\"Moldura capó\", 33:\"Moldura maletro\", 34:\"Moldura puerta delantera derecha\", 35:\"Moldura puerta delantera izquierda\", \n",
    "36:\"Moldura puerta trasera derecha\", 37:\"Moldura puerta trasera izquierda\", 38:\"Parabrisas delantero\", 39:\"Parabrisas trasero\", 40:\"Parachoques delantero\", 41:\"Parachoques trasero\", 42:\"Poste del techo\", 43:\"Puerta delantera derecha\", 44:\"Puerta delantera izquierda\", 45:\"Puerta trasera derecha\", 46:\"Puerta trasera izquierda\", 47:\"Radiador\", 48:\"Rejilla o parrilla\", 49:\"Rueda\", 50:\"Silenciador o mofle\", 51:\"Tapa de combustible\", 52:\"Tapa de rueda\", 53:\"Techo\", 54:\"Techo corredizo\", 55:\"Ventana delantera derecha\", 56:\"Ventana delantera izquierda\", 57:\"Ventana trasera derecha\", 58:\"Ventana trasera izquierda\", 59:\"Ventanilla delantera derecha\", 60:\"Ventanilla delantera izquierda\", 61:\"Ventanilla trasera derecha\", 62:\"Ventanilla trasera izquierda\", 63:\"Volante\"     \n",
    "\n",
    "2. Tipos de Daño:\n",
    "- 1:\"Abolladura\", 2:\"Arañazo\", 3:\"Corrosión\", 4:\"Deformación\", \n",
    "5:\"Desprendimiento\", 6:\"Fractura\", 7:\"Rayón\", 8:\"Rotura\"\n",
    "\n",
    "3. Sugerencia:\n",
    "- 1:\"Reparar\", \n",
    "2:\"Reemplazar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diccionarios completos: Todos los mapeos completos para piezas, daños y sugerencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. DICCIONARIOS COMPLETOS DE MAPEO (CORREGIDOS)\n",
    "# =============================================\n",
    "\n",
    "# Diccionario para Piezas del Vehículo (completo y corregido)\n",
    "label_to_cls_piezas = {\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Asiento\",\n",
    "    4: \"Brazo del techo\",\n",
    "    5: \"Brazo transversal\",\n",
    "    6: \"Capó\",\n",
    "    7: \"Cerradura capo\",\n",
    "    8: \"Cerradura maletero\",\n",
    "    9: \"Cerradura puerta\",\n",
    "    10: \"Espejo lateral derecho\",\n",
    "    11: \"Espejo lateral izquierdo\",\n",
    "    12: \"Espejo retrovisor\",\n",
    "    13: \"Faros derecho\",\n",
    "    14: \"Faros izquierdo\",\n",
    "    15: \"Guardabarros delantero derecho\",\n",
    "    16: \"Guardabarros delantero izquierdo\",\n",
    "    17: \"Guardabarros trasero derecho\",\n",
    "    18: \"Guardabarros trasero izquierdo\",\n",
    "    19: \"Limpiaparabrisas\",\n",
    "    20: \"Luz indicadora delantera derecha\",\n",
    "    21: \"Luz indicadora delantera izquierda\",\n",
    "    22: \"Luz indicadora trasera derecha\",\n",
    "    23: \"Luz indicadora trasera izquierda\",\n",
    "    24: \"Luz trasera derecho\",\n",
    "    25: \"Luz trasera izquierdo\",\n",
    "    26: \"Maletero\",\n",
    "    27: \"Manija derecha\",\n",
    "    28: \"Manija izquierda\",\n",
    "    29: \"Marco de la ventana\",\n",
    "    30: \"Marco de las puertas\",\n",
    "    31: \"Matrícula\",\n",
    "    32: \"Moldura capó\",\n",
    "    33: \"Moldura maletro\",\n",
    "    34: \"Moldura puerta delantera derecha\",\n",
    "    35: \"Moldura puerta delantera izquierda\",\n",
    "    36: \"Moldura puerta trasera derecha\",\n",
    "    37: \"Moldura puerta trasera izquierda\",\n",
    "    38: \"Parabrisas delantero\",\n",
    "    39: \"Parabrisas trasero\",\n",
    "    40: \"Parachoques delantero\",\n",
    "    41: \"Parachoques trasero\",\n",
    "    42: \"Poste del techo\",\n",
    "    43: \"Puerta delantera derecha\",\n",
    "    44: \"Puerta delantera izquierda\",\n",
    "    45: \"Puerta trasera derecha\",\n",
    "    46: \"Puerta trasera izquierda\",\n",
    "    47: \"Radiador\",\n",
    "    48: \"Rejilla, parrilla\",\n",
    "    49: \"Rueda\",\n",
    "    50: \"Silenciador, el mofle\",\n",
    "    51: \"Tapa de combustible\",\n",
    "    52: \"Tapa de rueda\",\n",
    "    53: \"Techo\",\n",
    "    54: \"Techo corredizo\",\n",
    "    55: \"Ventana delantera derecha\",\n",
    "    56: \"Ventana delantera izquierda\",\n",
    "    57: \"Ventana trasera derecha\",\n",
    "    58: \"Ventana trasera izquierda\",\n",
    "    59: \"Ventanilla delantera derecha\",\n",
    "    60: \"Ventanilla delantera izquierda\",\n",
    "    61: \"Ventanilla trasera derecha\",\n",
    "    62: \"Ventanilla trasera izquierda\",\n",
    "    63: \"Volante\"\n",
    "}\n",
    "\n",
    "# Diccionario para Tipos de Daño (completo)\n",
    "label_to_cls_danos = {\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Arañazo\",\n",
    "    3: \"Corrosión\",\n",
    "    4: \"Deformación\",\n",
    "    5: \"Desprendimiento\",\n",
    "    6: \"Fractura\",\n",
    "    7: \"Rayón\",\n",
    "    8: \"Rotura\"\n",
    "}\n",
    "\n",
    "# Diccionario para Sugerencia (completo)\n",
    "label_to_cls_sugerencia = {\n",
    "    1: \"Reparar\",\n",
    "    2: \"Reemplazar\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Procesamiento robusto:\n",
    "    - Limpieza de texto (minúsculas, eliminar espacios)\n",
    "    - Manejo de valores compuestos (ej. \"Abolladura-dent\")\n",
    "    - Manejo de errores y valores no encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionarios inversos (texto -> número)\n",
    "cls_to_label_piezas = {v.lower().strip(): k for k, v in label_to_cls_piezas.items()}\n",
    "cls_to_label_danos = {v.lower().strip(): k for k, v in label_to_cls_danos.items()}\n",
    "cls_to_label_sugerencia = {v.lower().strip(): k for k, v in label_to_cls_sugerencia.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Validación de datos:\n",
    "    - Detección y reporte de valores que no se pudieron mapear\n",
    "    - Verificación de conteo de registros\n",
    "\n",
    "4. Manejo de casos especiales:\n",
    "    - Valores nulos o faltantes\n",
    "    - Texto con formatos inconsistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. FUNCIONES DE PROCESAMIENTO\n",
    "# =============================================\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia y estandariza el texto para comparación\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    return texto.lower().strip()\n",
    "\n",
    "def procesar_dano(texto):\n",
    "    \"\"\"Procesa la columna Tipos de Daño (maneja casos como 'Abolladura-dent')\"\"\"\n",
    "    texto = limpiar_texto(texto)\n",
    "    if '-' in texto:\n",
    "        return texto.split('-')[0]\n",
    "    return texto\n",
    "\n",
    "def mapear_valor(texto, diccionario, columna):\n",
    "    \"\"\"Mapea texto a valor numérico con manejo de errores\"\"\"\n",
    "    try:\n",
    "        texto = limpiar_texto(texto)\n",
    "        if columna == 'Tipos de Daño':\n",
    "            texto = procesar_dano(texto)\n",
    "        return diccionario.get(texto, -1)  # -1 para valores no encontrados\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando valor: {texto} - {str(e)}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Metadatos útiles:\n",
    "    - Reporte de valores no encontrados\n",
    "    - Confirmación del proceso completado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. PROCESAR EL ARCHIVO CSV\n",
    "# =============================================\n",
    "\n",
    "# Leer el archivo original (ajusta el separador según tu archivo)\n",
    "df = pd.read_csv('data/fotos_siniestros/clasificacion_fotos_siniestros_partes.csv', sep='|')\n",
    "\n",
    "# Aplicar el mapeo a cada columna\n",
    "df['Tipos de Daño_encoded'] = df['Tipos de Daño'].apply(\n",
    "    lambda x: mapear_valor(x, cls_to_label_danos, 'Tipos de Daño'))\n",
    "df['Piezas del Vehículo_encoded'] = df['Piezas del Vehículo'].apply(\n",
    "    lambda x: mapear_valor(x, cls_to_label_piezas, 'Piezas del Vehículo'))\n",
    "df['Sugerencia_encoded'] = df['Sugerencia'].apply(\n",
    "    lambda x: mapear_valor(x, cls_to_label_sugerencia, 'Sugerencia'))\n",
    "\n",
    "# Verificar valores no mapeados (-1)\n",
    "for col in ['Tipos de Daño', 'Piezas del Vehículo', 'Sugerencia']:\n",
    "    no_encontrados = df[df[f'{col}_encoded'] == -1][col].unique()\n",
    "    if len(no_encontrados) > 0:\n",
    "        print(f\"\\nADVERTENCIA: Valores no mapeados en {col}:\")\n",
    "        print(no_encontrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. CREAR NUEVO CSV CON ETIQUETAS\n",
    "# =============================================\n",
    "\n",
    "# Seleccionar y renombrar columnas\n",
    "df_encoded = df[[\n",
    "    'Imagen',\n",
    "    'Tipos de Daño_encoded',\n",
    "    'Piezas del Vehículo_encoded',\n",
    "    'Sugerencia_encoded'\n",
    "]].rename(columns={\n",
    "    'Tipos de Daño_encoded': 'Tipos de Daño',\n",
    "    'Piezas del Vehículo_encoded': 'Piezas del Vehículo',\n",
    "    'Sugerencia_encoded': 'Sugerencia'\n",
    "})\n",
    "\n",
    "# Guardar el nuevo archivo CSV\n",
    "df_encoded.to_csv('data/fotos_siniestros/datos_vehiculos_encoded.csv', index=False, sep='|')\n",
    "\n",
    "print(\"\\nProceso completado exitosamente!\")\n",
    "print(f\"Archivo original: {len(df)} registros\")\n",
    "print(f\"Archivo codificado: {len(df_encoded)} registros\")\n",
    "print(\"Nuevo archivo guardado como: datos_vehiculos_encoded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los datos para entrenamientos y pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manejo de imágenes múltiples: Agrupa y consolida correctamente imágenes con múltiples registros (como 9.jpg y 15.jpg en tu ejemplo).\n",
    "\n",
    "- División estratificada: Mantiene proporciones similares de clases en todos los conjuntos, especialmente para la columna \"Sugerencia\".\n",
    "\n",
    "- Reconstrucción precisa: Después de la división, expande los registros para mantener la estructura original del CSV.\n",
    "\n",
    "- Validación incluida: Crea tres conjuntos (entrenamiento, validación y prueba) con proporciones 60%-20%-20%.\n",
    "\n",
    "- Reporte detallado: Genera estadísticas de distribución para cada conjunto.\n",
    "\n",
    "- Semilla aleatoria: Garantiza reproducibilidad en las divisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Configuración\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2  # 20% para prueba\n",
    "VAL_SIZE = 0.25  # 25% del entrenamiento para validación (20% del total)\n",
    "\n",
    "# 1. Cargar los datos codificados\n",
    "df = pd.read_csv('data/fotos_siniestros/datos_vehiculos_encoded.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Manejar imágenes con múltiples registros (como 9.jpg, 15.jpg)\n",
    "# Agrupamos por imagen y consolidamos las etiquetas\n",
    "def consolidar_etiquetas(group):\n",
    "    return pd.Series({\n",
    "        'Tipos de Daño': list(group['Tipos de Daño'].unique()),\n",
    "        'Piezas del Vehículo': list(group['Piezas del Vehículo'].unique()),\n",
    "        'Sugerencia': list(group['Sugerencia'].unique())\n",
    "    })\n",
    "\n",
    "df_consolidado = df.groupby('Imagen').apply(consolidar_etiquetas).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dividir los datos\n",
    "# Primero dividimos en entrenamiento+validación (80%) y prueba (20%)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df_consolidado,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df_consolidado['Sugerencia'].apply(lambda x: x[0])  # Estratificar por primera sugerencia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego dividimos el entrenamiento en entrenamiento (60%) y validación (20%)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=VAL_SIZE/(1-TEST_SIZE),  # Ajustar para que sea 20% del total\n",
    "    random_state=SEED,\n",
    "    stratify=train_val_df['Sugerencia'].apply(lambda x: x[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reconstruir los DataFrames originales (desagrupar las listas)\n",
    "def expandir_registros(df):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        for tipo in row['Tipos de Daño']:\n",
    "            for pieza in row['Piezas del Vehículo']:\n",
    "                for sugerencia in row['Sugerencia']:\n",
    "                    records.append({\n",
    "                        'Imagen': row['Imagen'],\n",
    "                        'Tipos de Daño': tipo,\n",
    "                        'Piezas del Vehículo': pieza,\n",
    "                        'Sugerencia': sugerencia\n",
    "                    })\n",
    "    return pd.DataFrame(records)# Aplicar oversampling con SMOTE\n",
    "\n",
    "# Aplicar oversampling con SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(df[['Tipos de Daño', 'Piezas del Vehículo']], df['Sugerencia'])\n",
    "\n",
    "train_expanded = expandir_registros(train_df)\n",
    "val_expanded = expandir_registros(val_df)\n",
    "test_expanded = expandir_registros(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Guardar los conjuntos de datos\n",
    "os.makedirs('data/fotos_siniestros/datasets', exist_ok=True)\n",
    "\n",
    "train_expanded.to_csv('data/fotos_siniestros/datasets/train.csv', index=False, sep='|')\n",
    "val_expanded.to_csv('data/fotos_siniestros/datasets/val.csv', index=False, sep='|')\n",
    "test_expanded.to_csv('data/fotos_siniestros/datasets/test.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generar reporte de distribución\n",
    "def generar_reporte(df, nombre):\n",
    "    print(f\"\\nDistribución en {nombre}:\")\n",
    "    print(f\"- Total registros: {len(df)}\")\n",
    "    print(f\"- Imágenes únicas: {df['Imagen'].nunique()}\")\n",
    "    print(\"Distribución de sugerencias:\")\n",
    "    print(df['Sugerencia'].value_counts(normalize=True))\n",
    "    print(\"\\nDistribución de tipos de daño:\")\n",
    "    print(df['Tipos de Daño'].value_counts(normalize=True))\n",
    "    print(\"\\nDistribución de piezas (top 10):\")\n",
    "    print(df['Piezas del Vehículo'].value_counts(normalize=True).head(10))\n",
    "\n",
    "generar_reporte(train_expanded, \"Entrenamiento\")\n",
    "generar_reporte(val_expanded, \"Validación\")\n",
    "generar_reporte(test_expanded, \"Prueba\")\n",
    "\n",
    "print(\"\\nProceso completado. Archivos guardados en:\")\n",
    "print(\"- data/fotos_siniestros/datasets/train.csv\")\n",
    "print(\"- data/fotos_siniestros/datasets/val.csv\")\n",
    "print(\"- data/fotos_siniestros/datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis exhaustivo de la distribución de las clases en cada categoría (Tipos de Daño, Piezas del Vehículo y Sugerencia).\n",
    "\n",
    "Esto permitira saber si el dataset esta balanceado o no, si no esta balanceado se pueden ajustar Para datasets desbalanceados se puede modificar el parámetro stratify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método 1: Análisis Estadístico Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('data/fotos_siniestros/datos_vehiculos_encoded.csv', sep='|')\n",
    "\n",
    "# 1. Análisis para 'Sugerencia'\n",
    "sugerencia_dist = df['Sugerencia'].value_counts(normalize=True)\n",
    "print(\"Distribución de Sugerencias:\")\n",
    "print(sugerencia_dist)\n",
    "\n",
    "# 2. Análisis para 'Tipos de Daño'\n",
    "danos_dist = df['Tipos de Daño'].value_counts(normalize=True)\n",
    "print(\"\\nDistribución de Tipos de Daño:\")\n",
    "print(danos_dist)\n",
    "\n",
    "# 3. Análisis para 'Piezas del Vehículo' (top 20)\n",
    "piezas_dist = df['Piezas del Vehículo'].value_counts(normalize=True)\n",
    "print(\"\\nDistribución de Piezas (Top 20):\")\n",
    "print(piezas_dist.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método 2: Visualización Gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el estilo de los gráficos\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Gráfico para Sugerencia\n",
    "plt.subplot(3, 1, 1)\n",
    "df['Sugerencia'].map({1: 'Reparar', 2: 'Reemplazar'}).value_counts().plot(kind='bar')\n",
    "plt.title('Distribución de Sugerencias')\n",
    "plt.xlabel('Sugerencia')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "# 2. Gráfico para Tipos de Daño\n",
    "plt.subplot(3, 1, 2)\n",
    "df['Tipos de Daño'].map(label_to_cls_danos).valuescikit-learn_counts().plot(kind='bar')\n",
    "plt.title('Distribución de Tipos de Daño')\n",
    "plt.xlabel('Tipo de Daño')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "# 3. Gráfico para Piezas (Top 15)\n",
    "plt.subplot(3, 1, 3)\n",
    "df['Piezas del Vehículo'].map(label_to_cls_piezas).value_counts().head(15).plot(kind='bar')\n",
    "plt.title('Distribución de Piezas (Top 15)')\n",
    "plt.xlabel('Pieza del Vehículo')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método 3: Análisis de Desbalanceo con Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calcular pesos de clase para cada categoría\n",
    "def calcular_desbalanceo(columna):\n",
    "    clases = df[columna].unique()\n",
    "    conteos = df[columna].value_counts().sort_index()\n",
    "    desbalanceo = conteos.max() / conteos.min()\n",
    "    pesos = class_weight.compute_class_weight('balanced', classes=clases, y=df[columna])\n",
    "    \n",
    "    print(f\"\\nAnálisis de desbalanceo para {columna}:\")\n",
    "    print(f\"Ratio máximo de desbalanceo: {desbalanceo:.2f}:1\")\n",
    "    print(\"Distribución absoluta:\")\n",
    "    print(conteos)\n",
    "    print(\"\\nDistribución porcentual:\")\n",
    "    print((conteos/len(df)*100).round(2))\n",
    "    print(\"\\nPesos sugeridos para balanceo:\")\n",
    "    print(dict(zip(clases, pesos)))\n",
    "\n",
    "calcular_desbalanceo('Sugerencia')\n",
    "calcular_desbalanceo('Tipos de Daño')\n",
    "calcular_desbalanceo('Piezas del Vehículo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sugerencia:\n",
    "    - Si una categoría (ej. \"Reparar\") tiene >70% de los datos, hay desbalanceo significativo.\n",
    "    - Ratio > 3:1 (ej. 75% vs 25%) se considera problemático.\n",
    "    \n",
    "    Distribución de Sugerencias:\n",
    "    - 1 -> Reparar ->    0.700812\n",
    "    - 2 -> Reemplazar ->   0.299188\n",
    "\n",
    "- Tipos de Daño:\n",
    "    - Busca categorías con menos del 5% de representación.\n",
    "    - Si el ratio entre el más común y el menos común es > 10:1, es crítico.\n",
    "\n",
    "- Piezas del Vehículo:\n",
    "    - Piezas con < 1% de aparición son candidatas a agruparse en \"Otras\".\n",
    "    - Si el top 5 piezas concentra >80% de los datos, hay alta desproporción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Mejoras clave implementadas:\n",
    "    1. Transfer Learning por Fases:\n",
    "        - Fase 1: Solo entrenamiento de cabezales (capas superiores)\n",
    "        - Fase 2: Fine-tuning parcial (últimas capas del backbone)\n",
    "        - Fase 3: Fine-tuning completo\n",
    "\n",
    "    2. Pérdidas Balanceadas Automáticas:\n",
    "        - BalancedFocalLoss que calcula pesos automáticamente basados en la distribución de clases\n",
    "        - Balance dinámico entre tareas durante el entrenamiento\n",
    "\n",
    "    3. Arquitectura Mejorada:\n",
    "        - Cabezales más profundos con normalización de capas\n",
    "        - Inicialización de pesos Kaiming para mejor convergencia\n",
    "        - Dropout más agresivo para prevenir overfitting\n",
    "\n",
    "    4. Aumentación de Datos Mejorada:\n",
    "        - Transformaciones más variadas y realistas\n",
    "        - Perspectiva aleatoria para simular diferentes ángulos\n",
    "\n",
    "    5. Optimización Avanzada:\n",
    "        - OneCycleLR para la fase inicial\n",
    "        - ReduceLROnPlateau para fases posteriores\n",
    "        - Weight decay aumentado para mejor regularización\n",
    "\n",
    "## Resultados Esperados:\n",
    "    - Accuracy Daños: 15% → 30-45%\n",
    "    - Accuracy Piezas: 6% → 15-25%\n",
    "    - F1 Sugerencia: 0.11 → 0.50-0.65\n",
    "    - Tiempo Entrenamiento: Reducción de ~8h a ~3-4h\n",
    "\n",
    "## Pasos Adicionales Recomendados:\n",
    "    - Implementar logging con TensorBoard\n",
    "    - Añadir early stopping basado en métricas de validación\n",
    "    - Probar ensamblado de modelos para las tareas más críticas\n",
    "\n",
    "Nota: Este código proporciona una base sólida para continuar mejorando el modelo de manera sistemática.\n",
    "\n",
    "## Cambios Clave:\n",
    "    1. Corrección del Error:\n",
    "        - Añadido el parámetro metrics requerido por ReduceLROnPlateau.step()\n",
    "        - Usamos una combinación ponderada de las métricas de validación como métrica para el scheduler\n",
    "\n",
    "    2. Mejoras Adicionales:\n",
    "        - Añadido is_reduce_lr para manejar ambos tipos de schedulers\n",
    "        - Mostrar métricas de entrenamiento y validación\n",
    "        - Mejor estructuración de las fases de entrenamiento\n",
    "\n",
    "    3. Optimización del Pipeline:\n",
    "        - Fase 1: 5 épocas solo cabezales\n",
    "        - Fase 2: 10 épocas con capas superiores descongeladas\n",
    "        - Fase 3: 10 épocas fine-tuning completo\n",
    "\n",
    "## Resultados Esperados:\n",
    "    - Mejor convergencia gracias al ajuste automático del learning rate\n",
    "    - Mayor estabilidad en el entrenamiento\n",
    "    - Mejor balance entre las tareas\n",
    "\n",
    "## Observaciones sobre tus Resultados Iniciales:\n",
    "    - Daños: Mejoró de 7% a 25% (buen progreso)\n",
    "    - Piezas: Sigue baja (1-2%), necesita más atención\n",
    "    - Sugerencia: Buen rendimiento (60%+)\n",
    "\n",
    "## Recomendaciones Adicionales:\n",
    "    - Para mejorar piezas:\n",
    "        - Aumentar el peso de esta tarea en la pérdida total\n",
    "        - Considerar agrupar clases raras de piezas\n",
    "        - Añadir más ejemplos de las clases menos representadas\n",
    "\n",
    "    - Para estabilizar el entrenamiento:\n",
    "        - Añadir gradient clipping:\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    - Para acelerar el entrenamiento:\n",
    "        - Usar Automatic Mixed Precision (AMP):\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models as torchvision_models  # Cambiado el nombre para evitar conflicto\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "## Diccionario para Piezas del Vehículo (completo y corregido)\n",
    "label_to_cls_piezas = {\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Asiento\",\n",
    "    4: \"Brazo del techo\",\n",
    "    5: \"Brazo transversal\",\n",
    "    6: \"Capó\",\n",
    "    7: \"Cerradura capo\",\n",
    "    8: \"Cerradura maletero\",\n",
    "    9: \"Cerradura puerta\",\n",
    "    10: \"Espejo lateral derecho\",\n",
    "    11: \"Espejo lateral izquierdo\",\n",
    "    12: \"Espejo retrovisor\",\n",
    "    13: \"Faros derecho\",\n",
    "    14: \"Faros izquierdo\",\n",
    "    15: \"Guardabarros delantero derecho\",\n",
    "    16: \"Guardabarros delantero izquierdo\",\n",
    "    17: \"Guardabarros trasero derecho\",\n",
    "    18: \"Guardabarros trasero izquierdo\",\n",
    "    19: \"Limpiaparabrisas\",\n",
    "    20: \"Luz indicadora delantera derecha\",\n",
    "    21: \"Luz indicadora delantera izquierda\",\n",
    "    22: \"Luz indicadora trasera derecha\",\n",
    "    23: \"Luz indicadora trasera izquierda\",\n",
    "    24: \"Luz trasera derecho\",\n",
    "    25: \"Luz trasera izquierdo\",\n",
    "    26: \"Maletero\",\n",
    "    27: \"Manija derecha\",\n",
    "    28: \"Manija izquierda\",\n",
    "    29: \"Marco de la ventana\",\n",
    "    30: \"Marco de las puertas\",\n",
    "    31: \"Matrícula\",\n",
    "    32: \"Moldura capó\",\n",
    "    33: \"Moldura maletro\",\n",
    "    34: \"Moldura puerta delantera derecha\",\n",
    "    35: \"Moldura puerta delantera izquierda\",\n",
    "    36: \"Moldura puerta trasera derecha\",\n",
    "    37: \"Moldura puerta trasera izquierda\",\n",
    "    38: \"Parabrisas delantero\",\n",
    "    39: \"Parabrisas trasero\",\n",
    "    40: \"Parachoques delantero\",\n",
    "    41: \"Parachoques trasero\",\n",
    "    42: \"Poste del techo\",\n",
    "    43: \"Puerta delantera derecha\",\n",
    "    44: \"Puerta delantera izquierda\",\n",
    "    45: \"Puerta trasera derecha\",\n",
    "    46: \"Puerta trasera izquierda\",\n",
    "    47: \"Radiador\",\n",
    "    48: \"Rejilla, parrilla\",\n",
    "    49: \"Rueda\",\n",
    "    50: \"Silenciador, el mofle\",\n",
    "    51: \"Tapa de combustible\",\n",
    "    52: \"Tapa de rueda\",\n",
    "    53: \"Techo\",\n",
    "    54: \"Techo corredizo\",\n",
    "    55: \"Ventana delantera derecha\",\n",
    "    56: \"Ventana delantera izquierda\",\n",
    "    57: \"Ventana trasera derecha\",\n",
    "    58: \"Ventana trasera izquierda\",\n",
    "    59: \"Ventanilla delantera derecha\",\n",
    "    60: \"Ventanilla delantera izquierda\",\n",
    "    61: \"Ventanilla trasera derecha\",\n",
    "    62: \"Ventanilla trasera izquierda\",\n",
    "    63: \"Volante\"\n",
    "}\n",
    "\n",
    "## Diccionario para Tipos de Daño (completo)\n",
    "label_to_cls_danos = {\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Arañazo\",\n",
    "    3: \"Corrosión\",\n",
    "    4: \"Deformación\",\n",
    "    5: \"Desprendimiento\",\n",
    "    6: \"Fractura\",\n",
    "    7: \"Rayón\",\n",
    "    8: \"Rotura\"\n",
    "}\n",
    "\n",
    "## Diccionario para Sugerencia (completo)\n",
    "label_to_cls_sugerencia = {\n",
    "    1: \"Reparar\",\n",
    "    2: \"Reemplazar\"\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# =============================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "batch_size = 32\n",
    "\n",
    "# =============================================\n",
    "# 2. DATASET Y TRANSFORMACIONES MEJORADAS\n",
    "# =============================================\n",
    "class VehiculoDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, sep='|')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_distribution = self._compute_class_distribution()\n",
    "        \n",
    "    def _compute_class_distribution(self):\n",
    "        return {\n",
    "            'dano': Counter(self.data.iloc[:, 1]),\n",
    "            'pieza': Counter(self.data.iloc[:, 2]),\n",
    "            'sugerencia': Counter(self.data.iloc[:, 3])\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        labels = {\n",
    "            'dano': torch.tensor(self.data.iloc[idx, 1] - 1, dtype=torch.long),\n",
    "            'pieza': torch.tensor(self.data.iloc[idx, 2] - 1, dtype=torch.long),\n",
    "            'sugerencia': torch.tensor(self.data.iloc[idx, 3] - 1, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, labels\n",
    "\n",
    "# Transformaciones mejoradas con aumentación más agresiva\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# 3. MODELO MEJORADO CON TRANSFER LEARNING EN FASES\n",
    "# =============================================\n",
    "class EnhancedMultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_danos, num_piezas, num_sugerencias):\n",
    "        super().__init__()\n",
    "        # Backbone principal (congelado inicialmente)\n",
    "        self.base_model = torchvision_models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Congelar todos los parámetros inicialmente\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Capas compartidas\n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.7)\n",
    "        )\n",
    "        \n",
    "        # Cabezales específicos con mayor capacidad\n",
    "        self.dano_head = nn.Sequential(\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(768, num_danos)\n",
    "        )\n",
    "        \n",
    "        self.pieza_head = nn.Sequential(\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.LayerNorm(768),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(768, num_piezas)\n",
    "        )\n",
    "        \n",
    "        self.sugerencia_head = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_sugerencias)\n",
    "        )\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for head in [self.dano_head, self.pieza_head, self.sugerencia_head]:\n",
    "            for m in head.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def unfreeze_backbone(self, unfreeze_layers=5):\n",
    "        \"\"\"Descongela capas superiores del backbone\"\"\"\n",
    "        total_layers = len(list(self.base_model.parameters()))\n",
    "        for i, param in enumerate(self.base_model.parameters()):\n",
    "            if i >= total_layers - unfreeze_layers:\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_features = self.base_model(x)\n",
    "        shared = self.shared_features(base_features)\n",
    "        \n",
    "        return {\n",
    "            'dano': self.dano_head(shared),\n",
    "            'pieza': self.pieza_head(shared),\n",
    "            'sugerencia': self.sugerencia_head(shared)\n",
    "        }\n",
    "\n",
    "# =============================================\n",
    "# 4. FUNCIONES DE PÉRDIDA MEJORADAS\n",
    "# =============================================\n",
    "class BalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha='auto', gamma=2.0, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha == 'auto' and num_classes:\n",
    "            self.alpha = torch.ones(num_classes, device=device) / num_classes\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha[targets]\n",
    "            loss = alpha * (1-pt)**self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1-pt)**self.gamma * ce_loss\n",
    "            \n",
    "        return loss.mean()\n",
    "\n",
    "def create_criterion(dataset, device):\n",
    "    # Calcular pesos automáticamente\n",
    "    dano_weights = 1.0 / torch.tensor(\n",
    "        [dataset.class_distribution['dano'][i+1] for i in range(8)], \n",
    "        device=device\n",
    "    )\n",
    "    dano_weights = dano_weights / dano_weights.sum()\n",
    "    \n",
    "    pieza_weights = 1.0 / torch.tensor(\n",
    "        [dataset.class_distribution['pieza'][i+1] for i in range(63)], \n",
    "        device=device\n",
    "    )\n",
    "    pieza_weights = pieza_weights / pieza_weights.sum()\n",
    "    \n",
    "    return {\n",
    "        'dano': BalancedFocalLoss(alpha=dano_weights, gamma=2),\n",
    "        'pieza': BalancedFocalLoss(alpha=pieza_weights, gamma=2),\n",
    "        'sugerencia': BalancedFocalLoss(alpha=torch.tensor([0.4, 0.6], device=device))\n",
    "    }\n",
    "\n",
    "# =============================================\n",
    "# 5. ENTRENAMIENTO POR ETAPAS (VERSIÓN CORREGIDA)\n",
    "# =============================================\n",
    "def train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, phase_name, is_reduce_lr=False):\n",
    "    print(f\"\\n=== Fase: {phase_name} ===\")\n",
    "    best_metrics = {'dano': 0, 'pieza': 0, 'sugerencia': 0}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_metrics = {task: 0 for task in best_metrics}\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = {k: v.to(device) for k, v in labels.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Pérdida adaptativa\n",
    "            losses = {\n",
    "                task: criterion[task](outputs[task], labels[task]) \n",
    "                for task in best_metrics\n",
    "            }\n",
    "            \n",
    "            # Balance dinámico de tareas\n",
    "            task_weights = {\n",
    "                'dano': 0.5 * (1 - best_metrics['dano']),\n",
    "                'pieza': 0.3 * (1 - best_metrics['pieza']),\n",
    "                'sugerencia': 0.2 * (1 - best_metrics['sugerencia'])\n",
    "            }\n",
    "            total_loss = sum(w * losses[task] for task, w in task_weights.items())\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Métricas\n",
    "            for task in best_metrics:\n",
    "                _, preds = torch.max(outputs[task], 1)\n",
    "                train_metrics[task] += (preds == labels[task]).sum().item()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_metrics = {task: 0 for task in best_metrics}\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = {k: v.to(device) for k, v in labels.items()}\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                for task in best_metrics:\n",
    "                    _, preds = torch.max(outputs[task], 1)\n",
    "                    val_metrics[task] += (preds == labels[task]).sum().item()\n",
    "        \n",
    "        # Actualizar scheduler\n",
    "        if is_reduce_lr:\n",
    "            # Usamos el promedio ponderado de las métricas de validación\n",
    "            scheduler.step(0.5*val_metrics['dano'] + 0.3*val_metrics['pieza'] + 0.2*val_metrics['sugerencia'])\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Resultados\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        for task in best_metrics:\n",
    "            val_acc = val_metrics[task] / total_samples\n",
    "            train_acc = train_metrics[task] / len(train_loader.dataset)\n",
    "            print(f\"{task.capitalize()} - Train: {train_acc:.4f}, Val: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > best_metrics[task]:\n",
    "                best_metrics[task] = val_acc\n",
    "                torch.save(model.state_dict(), f'best_{task}_model.pth')\n",
    "    \n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VehiculosVerificationDeDannosEtiquetas/.venv/lib64/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/Python/VehiculosVerificationDeDannosEtiquetas/.venv/lib64/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fase: Entrenamiento inicial (cabezales) ===\n",
      "\n",
      "Epoch 1/5\n",
      "Dano - Train: 0.1767, Val: 0.1626\n",
      "Pieza - Train: 0.0175, Val: 0.0191\n",
      "Sugerencia - Train: 0.5023, Val: 0.6446\n",
      "\n",
      "Epoch 2/5\n",
      "Dano - Train: 0.1490, Val: 0.1726\n",
      "Pieza - Train: 0.0143, Val: 0.0202\n",
      "Sugerencia - Train: 0.5470, Val: 0.6043\n",
      "\n",
      "Epoch 3/5\n",
      "Dano - Train: 0.1905, Val: 0.1513\n",
      "Pieza - Train: 0.0171, Val: 0.0191\n",
      "Sugerencia - Train: 0.5494, Val: 0.5908\n",
      "\n",
      "Epoch 4/5\n",
      "Dano - Train: 0.1933, Val: 0.2096\n",
      "Pieza - Train: 0.0189, Val: 0.0224\n",
      "Sugerencia - Train: 0.5507, Val: 0.5863\n",
      "\n",
      "Epoch 5/5\n",
      "Dano - Train: 0.2006, Val: 0.1648\n",
      "Pieza - Train: 0.0157, Val: 0.0146\n",
      "Sugerencia - Train: 0.5521, Val: 0.5751\n",
      "\n",
      "=== Fase: Fine-tuning parcial ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VehiculosVerificationDeDannosEtiquetas/.venv/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Dano - Train: 0.2136, Val: 0.2657\n",
      "Pieza - Train: 0.0258, Val: 0.0157\n",
      "Sugerencia - Train: 0.5254, Val: 0.5807\n",
      "\n",
      "Epoch 2/10\n",
      "Dano - Train: 0.2034, Val: 0.1917\n",
      "Pieza - Train: 0.0212, Val: 0.0101\n",
      "Sugerencia - Train: 0.5438, Val: 0.5818\n",
      "\n",
      "Epoch 3/10\n",
      "Dano - Train: 0.2108, Val: 0.2141\n",
      "Pieza - Train: 0.0221, Val: 0.0135\n",
      "Sugerencia - Train: 0.5526, Val: 0.6312\n",
      "\n",
      "Epoch 4/10\n",
      "Dano - Train: 0.2159, Val: 0.2567\n",
      "Pieza - Train: 0.0157, Val: 0.0112\n",
      "Sugerencia - Train: 0.5807, Val: 0.5303\n",
      "\n",
      "Epoch 5/10\n",
      "Dano - Train: 0.2228, Val: 0.2343\n",
      "Pieza - Train: 0.0171, Val: 0.0056\n",
      "Sugerencia - Train: 0.5498, Val: 0.6357\n",
      "\n",
      "Epoch 6/10\n",
      "Dano - Train: 0.2071, Val: 0.2399\n",
      "Pieza - Train: 0.0175, Val: 0.0078\n",
      "Sugerencia - Train: 0.5558, Val: 0.5987\n",
      "\n",
      "Epoch 7/10\n",
      "Dano - Train: 0.2182, Val: 0.2534\n",
      "Pieza - Train: 0.0148, Val: 0.0078\n",
      "Sugerencia - Train: 0.5323, Val: 0.6289\n",
      "\n",
      "Epoch 8/10\n",
      "Dano - Train: 0.2352, Val: 0.2713\n",
      "Pieza - Train: 0.0171, Val: 0.0067\n",
      "Sugerencia - Train: 0.5618, Val: 0.6222\n",
      "\n",
      "Epoch 9/10\n",
      "Dano - Train: 0.2348, Val: 0.2489\n",
      "Pieza - Train: 0.0171, Val: 0.0112\n",
      "Sugerencia - Train: 0.5457, Val: 0.6289\n",
      "\n",
      "Epoch 10/10\n",
      "Dano - Train: 0.2611, Val: 0.2724\n",
      "Pieza - Train: 0.0175, Val: 0.0090\n",
      "Sugerencia - Train: 0.5558, Val: 0.6155\n",
      "\n",
      "=== Fase: Fine-tuning completo ===\n",
      "\n",
      "Epoch 1/10\n",
      "Dano - Train: 0.2375, Val: 0.2466\n",
      "Pieza - Train: 0.0185, Val: 0.0146\n",
      "Sugerencia - Train: 0.5724, Val: 0.4742\n",
      "\n",
      "Epoch 2/10\n",
      "Dano - Train: 0.2352, Val: 0.2130\n",
      "Pieza - Train: 0.0185, Val: 0.0146\n",
      "Sugerencia - Train: 0.5655, Val: 0.6031\n",
      "\n",
      "Epoch 3/10\n",
      "Dano - Train: 0.2352, Val: 0.2186\n",
      "Pieza - Train: 0.0180, Val: 0.0157\n",
      "Sugerencia - Train: 0.5664, Val: 0.6256\n",
      "\n",
      "Epoch 4/10\n",
      "Dano - Train: 0.2417, Val: 0.2186\n",
      "Pieza - Train: 0.0194, Val: 0.0168\n",
      "Sugerencia - Train: 0.5567, Val: 0.6233\n",
      "\n",
      "Epoch 5/10\n",
      "Dano - Train: 0.2546, Val: 0.2175\n",
      "Pieza - Train: 0.0254, Val: 0.0146\n",
      "Sugerencia - Train: 0.5812, Val: 0.6312\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 6. PIPELINE COMPLETO (VERSIÓN CORREGIDA)\n",
    "# =============================================\n",
    "# Last Execution 12:45:57 PM\n",
    "# Execution Time 56m 19.4s\n",
    "# Overhead Time 104ms\n",
    "# Render Times\n",
    "# VS Code Builtin Notebook Output Renderer 8ms\n",
    "\n",
    "def main():\n",
    "    # Cargar datos\n",
    "    image_dir = 'data/fotos_siniestros/'\n",
    "    train_dataset = VehiculoDataset(\n",
    "        csv_file='data/fotos_siniestros/datasets/train.csv',\n",
    "        root_dir=image_dir,\n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    val_dataset = VehiculoDataset(\n",
    "        csv_file='data/fotos_siniestros/datasets/val.csv',\n",
    "        root_dir=image_dir,\n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = EnhancedMultiTaskModel(8, 63, 2).to(device)\n",
    "    criterion = create_criterion(train_dataset, device)\n",
    "    \n",
    "    # Fase 1: Solo cabezales (5 épocas)\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.shared_features.parameters()},\n",
    "        {'params': model.dano_head.parameters()},\n",
    "        {'params': model.pieza_head.parameters()},\n",
    "        {'params': model.sugerencia_head.parameters()}\n",
    "    ], lr=0.001, weight_decay=1e-3)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=0.01, \n",
    "        steps_per_epoch=len(train_loader), \n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, 5, \"Entrenamiento inicial (cabezales)\")\n",
    "    \n",
    "    # Fase 2: Descongelar capas superiores (10 épocas)\n",
    "    model.unfreeze_backbone(unfreeze_layers=10)\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.base_model.parameters(), 'lr': 0.0001},\n",
    "        {'params': model.shared_features.parameters(), 'lr': 0.0005},\n",
    "        {'params': model.dano_head.parameters(), 'lr': 0.001},\n",
    "        {'params': model.pieza_head.parameters(), 'lr': 0.001},\n",
    "        {'params': model.sugerencia_head.parameters(), 'lr': 0.005}\n",
    "    ], weight_decay=1e-3)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.5, \n",
    "        patience=2, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, 10, \"Fine-tuning parcial\", is_reduce_lr=True)\n",
    "    \n",
    "    # Fase 3: Fine-tuning completo (10 épocas)\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    train_phase(model, train_loader, val_loader, criterion, optimizer, scheduler, 10, \"Fine-tuning completo\", is_reduce_lr=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# EVALUACIÓN FINAL Y MATRICES DE CONFUSIÓN\n",
    "# =============================================\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Matriz de Confusión', cmap=plt.cm.Blues, figsize=(12, 10)):\n",
    "    \"\"\"Función mejorada para visualizar matrices de confusión\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar=False, linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.xlabel('Predicción', fontsize=12)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar la figura\n",
    "    filename = title.lower().replace(' ', '_') + '.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_task(model, loader, task_name, label_dict):\n",
    "    \"\"\"Evalúa un modelo en una tarea específica y genera métricas\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs[task_name], 1)\n",
    "            y_true.extend(labels[task_name].cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Filtrar solo clases presentes\n",
    "    present_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    class_names = [label_dict[label+1] for label in present_labels]\n",
    "    \n",
    "    # Manejo especial para piezas (muchas clases)\n",
    "    if task_name == 'pieza' and len(present_labels) > 20:\n",
    "        label_counts = {label: np.sum(y_true == label) for label in present_labels}\n",
    "        top_labels = sorted(present_labels, key=lambda x: label_counts[x], reverse=True)[:20]\n",
    "        mask = np.isin(y_true, top_labels)\n",
    "        y_true_filtered = y_true[mask]\n",
    "        y_pred_filtered = y_pred[mask]\n",
    "        class_names = [label_dict[label+1] for label in top_labels]\n",
    "    else:\n",
    "        y_true_filtered = y_true\n",
    "        y_pred_filtered = y_pred\n",
    "    \n",
    "    print(f\"\\n=== Resultados para {task_name.capitalize()} ===\")\n",
    "    print(classification_report(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        target_names=list(label_dict.values()),\n",
    "        zero_division=0,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    plot_confusion_matrix(\n",
    "        y_true_filtered, \n",
    "        y_pred_filtered, \n",
    "        classes=class_names,\n",
    "        title=f'Matriz de Confusión - {task_name.capitalize()}',\n",
    "        figsize=(15, 12) if task_name == 'pieza' else (10, 8)\n",
    "    )\n",
    "\n",
    "# Diccionarios de etiquetas\n",
    "label_dicts = {\n",
    "    'dano': label_to_cls_danos,\n",
    "    'pieza': label_to_cls_piezas,\n",
    "    'sugerencia': label_to_cls_sugerencia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# CARGA Y EVALUACIÓN DE MODELOS ENTRENADOS\n",
    "# =============================================\n",
    "def load_and_evaluate_models():\n",
    "    \"\"\"Carga los modelos entrenados y genera las matrices de confusión\"\"\"\n",
    "    # Cargar los mejores modelos para cada tarea\n",
    "    tasks = ['dano', 'pieza', 'sugerencia']\n",
    "    model_instances = {}  # Cambio de nombre para evitar conflicto\n",
    "    \n",
    "    for task in tasks:\n",
    "        model_path = f'best_{task}_model.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            # Crear nueva instancia del modelo\n",
    "            model = EnhancedMultiTaskModel(8, 63, 2).to(device)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model_instances[task] = model\n",
    "            print(f\"Modelo para {task} cargado exitosamente\")\n",
    "        else:\n",
    "            print(f\"Advertencia: No se encontró el modelo para {task} en {model_path}\")\n",
    "    \n",
    "    # Verificar si val_loader está definido\n",
    "    if 'val_loader' not in locals():\n",
    "        val_dataset = VehiculoDataset(\n",
    "            csv_file='data/fotos_siniestros/datasets/val.csv',\n",
    "            root_dir='data/fotos_siniestros/',\n",
    "            transform=data_transforms['val']\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    # Evaluar cada tarea\n",
    "    for task in tasks:\n",
    "        if task in model_instances:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Evaluando modelo para tarea: {task.upper()}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            evaluate_task(model_instances[task], val_loader, task, label_dicts[task])\n",
    "\n",
    "# Ejecutar la evaluación\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
